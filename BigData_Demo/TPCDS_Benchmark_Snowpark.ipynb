{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0429472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from snowflake.snowpark import Session\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "\n",
    "with open('../../creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_WH = data['warehouse']\n",
    "    passphrase = data['passphrase']\n",
    "\n",
    "# Read the private key from the .p8 file\n",
    "with open('../../rsa_key.p8', 'rb') as key_file:\n",
    "    private_key = key_file.read()\n",
    "\n",
    "# If the private key is encrypted, load it with a passphrase\n",
    "# Replace 'your_key_passphrase' with your actual passphrase if needed\n",
    "private_key_obj = serialization.load_pem_private_key(\n",
    "    private_key,\n",
    "    password=passphrase.encode() if passphrase else None,\n",
    "    backend=default_backend()\n",
    ")\n",
    "\n",
    "# Define connection parameters including the private key\n",
    "CONNECTION_PARAMETERS = {\n",
    "    'user': USERNAME,\n",
    "    'account': SF_ACCOUNT,\n",
    "    'private_key': private_key_obj,\n",
    "    'warehouse': SF_WH,\n",
    "}\n",
    "\n",
    "# Create a session with the specified connection parameters\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core import Root\n",
    "root = Root(session)\n",
    "from snowflake.snowpark.functions import col \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [
    {
     "ename": "SnowparkSessionException",
     "evalue": "(1403): No default Session is found. Please create a session before you call function 'udf' or use decorator '@udf'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSnowparkSessionException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# We can also use Snowpark for our analyses!\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_active_session\n\u001b[0;32m----> 7\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mget_active_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/working311/lib/python3.11/site-packages/snowflake/snowpark/context.py:32\u001b[0m, in \u001b[0;36mget_active_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_active_session\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnowflake.snowpark.Session\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the current active Snowpark session.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Raises: SnowparkSessionException: If there is more than one active session or no active sessions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m        A :class:`Session` object for the current session.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msnowflake\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msnowpark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_active_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/working311/lib/python3.11/site-packages/snowflake/snowpark/session.py:212\u001b[0m, in \u001b[0;36m_get_active_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mMORE_THAN_ONE_ACTIVE_SESSIONS()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_NO_DEFAULT_SESSION()\n",
      "\u001b[0;31mSnowparkSessionException\u001b[0m: (1403): No default Session is found. Please create a session before you call function 'udf' or use decorator '@udf'."
     ]
    }
   ],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "#session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db49c9a-73ed-498f-bda6-3ecc8852e89d",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.preprocessing import KBinsDiscretizer, OneHotEncoder\n",
    "from snowflake.ml.modeling.impute import SimpleImputer\n",
    "\n",
    "from snowflake.ml.modeling.compose import ColumnTransformer\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from snowflake.ml.modeling.impute import SimpleImputer\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "\n",
    "import snowflake.snowpark.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746c0225-2829-4c79-b4d2-eb094ea38089",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "#!pip install xgboost==2.0.3\n",
    "\n",
    "from time import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbb00a9-bbc2-4434-bb53-4eda0a899272",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"TOTAL_SALES\"  |\"C_BIRTH_YEAR\"  |\"CD_GENDER\"  |\"CD_MARITAL_STATUS\"  |\"CD_CREDIT_RATING\"  |\"CD_EDUCATION_STATUS\"  |\"CD_DEP_COUNT\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "|30368.88       |1972            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "|39077.29       |1976            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "|33980.60       |1976            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "|40226.64       |1949            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "|25446.41       |1952            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "|34458.32       |1982            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "|28561.76       |1942            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "|32611.44       |1978            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "|32713.36       |1939            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "|34231.25       |1951            |M            |U                    |Good                |4 yr Degree            |0               |\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snowdf = session.table(\"tpcds_xgboost.demo.feature_store\")\n",
    "snowdf = snowdf.drop(['CA_ZIP','CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "snowdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b095078-c25f-4d24-a18e-16d6189136e2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "## Dropping any null values\n",
    "from snowflake.snowpark.functions import col, is_null\n",
    "\n",
    "# Create a filter condition for non-finite values across all columns\n",
    "non_finite_filter = None\n",
    "\n",
    "# Iterate over all columns and update the filter condition\n",
    "for column in snowdf.columns:\n",
    "    current_filter = is_null(col(column))\n",
    "    non_finite_filter = current_filter if non_finite_filter is None else (non_finite_filter | current_filter)\n",
    "\n",
    "# Apply the filter to the DataFrame to exclude rows with any non-finite values\n",
    "df_filtered = snowdf.filter(~non_finite_filter)\n",
    "\n",
    "\n",
    "## Clean up cats\n",
    "def fix_values(columnn):\n",
    "    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n",
    "categorical_cols = ['CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "for col in categorical_cols:\n",
    "    df_filtered = df_filtered.with_column(col, fix_values(col))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276e412f-0fae-4315-bedc-9105993835b2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = df_filtered.columns\n",
    "feature_cols.remove('TOTAL_SALES')\n",
    "target_col = 'TOTAL_SALES'\n",
    "\n",
    "snowdf_train, snowdf_test = df_filtered.random_split([0.8, 0.2], seed=82) \n",
    "snowdf_train=snowdf_train.limit(1_000)\n",
    "snowdf_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47af0eba-7964-432a-a9fc-8cbd2e58135d",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    " ## Distributed Preprocessing - 25X to 50X faster\n",
    "\n",
    "numeric_features = ['C_BIRTH_YEAR', 'CD_DEP_COUNT']\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "categorical_cols = ['CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', XGBRegressor())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8ad1a8-15b8-4ec8-81d2-d34c63c80e65",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package 'fastparquet' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
      "The version of package 'cachetools' in the local environment is 5.3.2, which does not fit the criteria for the requirement 'cachetools<6'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.ml.modeling.model_selection.grid_search_cv.GridSearchCV at 0x169d007d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## Distributed HyperParameter Optimization\n",
    "hyper_param = dict(\n",
    "        model__max_depth=[2,4],\n",
    "        model__learning_rate=[0.1,0.3],\n",
    "    )\n",
    "\n",
    "xg_model = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=hyper_param,\n",
    "    #cv=5,\n",
    "    input_cols=numeric_features + categorical_cols,\n",
    "    label_cols=['TOTAL_SALES'],\n",
    "    output_cols=[\"TOTAL_SALES_PREDICT\"],\n",
    "    #verbose=4  ##verbose not working\n",
    ")\n",
    "\n",
    "# Fit and Score\n",
    "xg_model.fit(snowdf_train)\n",
    "##Takes 25 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34525ba4-f851-41c5-8b45-0b8f7a2760eb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "session.sql('ALTER SESSION SET USE_CACHED_RESULT=FALSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898389d-e1a7-428b-9f50-e5addabf25bd",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "lengths = [1_000_000,5_000_000,10_000_000,25_000_000,50_000_000]\n",
    "#lengths = [10_000_000,25_000_000,50_000_000]\n",
    "random.seed(9001)\n",
    "\n",
    "for i, length in enumerate(lengths):\n",
    "    seedv = random.randint(1, 1000)\n",
    "    snowdf_train, snowdf_test = df_filtered.random_split([0.8, 0.2], seed=seedv)  #82\n",
    "    snowdf_train=snowdf_train.limit(length)\n",
    "    print (snowdf_train.count())\n",
    "    init = time()\n",
    "    xg_model.fit(snowdf_train)\n",
    "    total_time = (time() - init) / 60\n",
    "    print(f'total rows: {length} total time: {total_time} seed: {seedv}')\n",
    "    snowdf_train = session.create_dataframe([1, 2, 3, 4]).to_df(\"a\")\n",
    "    snowdf_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697c3552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"SNOWPARK_OPT_WH\"\n",
      "build warehouse  MEDIUM\n",
      "prepping data for  1000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "1000000\n",
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajishah/anaconda3/envs/working311/lib/python3.11/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py:530: UserWarning: Warning: The Decimal(31, 2) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
      "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(self.dataset)\n",
      "Package 'snowflake-telemetry-python' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 1000000 total time: 1.5104982813199361 seed: 830\n",
      "prepping data for  25000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "25000000\n",
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajishah/anaconda3/envs/working311/lib/python3.11/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py:530: UserWarning: Warning: The Decimal(31, 2) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
      "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(self.dataset)\n",
      "Package 'snowflake-telemetry-python' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 25000000 total time: 9.28427733182907 seed: 386\n",
      "build warehouse  LARGE\n",
      "prepping data for  1000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "1000000\n",
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package 'fastparquet' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
      "The version of package 'cachetools' in the local environment is 5.3.2, which does not fit the criteria for the requirement 'cachetools<6'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 1000000 total time: 2.9549667199452716 seed: 157\n",
      "prepping data for  25000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "25000000\n",
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package 'fastparquet' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
      "The version of package 'cachetools' in the local environment is 5.3.2, which does not fit the criteria for the requirement 'cachetools<6'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 25000000 total time: 16.84705551067988 seed: 100\n",
      "build warehouse  XLARGE\n",
      "prepping data for  1000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "1000000\n",
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package 'fastparquet' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
      "The version of package 'cachetools' in the local environment is 5.3.2, which does not fit the criteria for the requirement 'cachetools<6'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 1000000 total time: 3.029693067073822 seed: 920\n",
      "prepping data for  25000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "25000000\n",
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package 'fastparquet' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
      "The version of package 'cachetools' in the local environment is 5.3.2, which does not fit the criteria for the requirement 'cachetools<6'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 25000000 total time: 16.91207328637441 seed: 735\n",
      "build warehouse  XXLARGE\n",
      "prepping data for  1000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "1000000\n",
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package 'fastparquet' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
      "The version of package 'cachetools' in the local environment is 5.3.2, which does not fit the criteria for the requirement 'cachetools<6'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 1000000 total time: 3.0922807335853575 seed: 353\n",
      "prepping data for  25000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "25000000\n",
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package 'fastparquet' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
      "The version of package 'cachetools' in the local environment is 5.3.2, which does not fit the criteria for the requirement 'cachetools<6'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 25000000 total time: 17.462916429837545 seed: 812\n"
     ]
    }
   ],
   "source": [
    "# An evaluation loop to see how the model does at the different series lengths\n",
    "lengths = [10_000, 50_000, 100_000, 500_000, 1_000_000, 2_000_000]\n",
    "#lengths = [1_000]\n",
    "lengths = [1_000_000, 25_000_000]\n",
    "TEST_ID = 2\n",
    "train_df = session.table('TPCDS_XGBOOST.DEMO.SERIES2M')\n",
    "print(session.get_current_warehouse())\n",
    "model_name = \"xgboost_approx\"\n",
    "dataset_name = \"TPCDS\"\n",
    "\n",
    "WAREHOUSE_SIZES = [\n",
    "    # \"XSMALL\",\n",
    "   # \"SMALL\",\n",
    "    \"MEDIUM\",\n",
    "    \"LARGE\",\n",
    "    \"XLARGE\",\n",
    "  \"XXLARGE\",\n",
    "]\n",
    "\n",
    "for whouse in WAREHOUSE_SIZES:  \n",
    "  print (\"build warehouse \", whouse)\n",
    "  wh_size = whouse\n",
    "  wh_type = \"SNOWPARK-OPTIMIZED\"\n",
    "  #wh_type = \"STANDARD\"\n",
    "\n",
    "  session.sql(\n",
    "  f\"\"\"CREATE OR REPLACE WAREHOUSE {\"snowpark_opt_wh\"}\n",
    "          WITH\n",
    "              WAREHOUSE_SIZE= '{wh_size}'\n",
    "              WAREHOUSE_TYPE = '{wh_type}'\n",
    "              AUTO_SUSPEND = 60\n",
    "              AUTO_RESUME = TRUE\n",
    "              INITIALLY_SUSPENDED = FALSE\n",
    "              MAX_CONCURRENCY_LEVEL = 1\n",
    "              MIN_CLUSTER_COUNT = 1\n",
    "              MAX_CLUSTER_COUNT = 1\n",
    "  \"\"\"\n",
    "  ).collect()\n",
    "\n",
    "  for length in lengths:\n",
    "    print (\"prepping data for \", length)\n",
    "    print(session.get_current_warehouse())\n",
    "    seedv = random.randint(1, 1000)\n",
    "    snowdf_train, snowdf_test = df_filtered.random_split([0.8, 0.2], seed=seedv)  #82\n",
    "    snowdf_train=snowdf_train.limit(length)\n",
    "    print (snowdf_train.count())\n",
    "    print (\"starting training\")\n",
    "    init = time()\n",
    "    # Run the regression model\n",
    "    xg_model.fit(snowdf_train)\n",
    "    total_time = (time() - init) / 60\n",
    "    print(f'total rows: {length} total time: {total_time} seed: {seedv}')\n",
    "    query_id = session.sql(\"SELECT LAST_QUERY_ID()\").collect()[0].as_dict()[\"LAST_QUERY_ID()\"]\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO TPCDS_XGBOOST.DEMO.RESULTS VALUES (\n",
    "      '{TEST_ID}',\n",
    "      '{model_name}',\n",
    "      '{query_id}',\n",
    "      '{length}',\n",
    "      '{total_time}',\n",
    "      '{dataset_name}',\n",
    "      '{wh_size}',\n",
    "      '{wh_type}'\n",
    "      )\n",
    "    \"\"\"\n",
    "    session.sql(query).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
