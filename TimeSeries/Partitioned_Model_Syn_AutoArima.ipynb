{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2452b045-2bcc-4df1-bfdf-6888c8320ed5",
   "metadata": {},
   "source": [
    "# Partitioned Custom Time Series Model - Nixta AutoArima\n",
    "\n",
    "This notebook shows how to partition a time series model. It uses synthetic data and Nixta AutoArima. The notebooks shows how to test it locally and then run the model in a distributed fashion in Snowflake. I have also made it so you can push the datasets into a Snowflake table for running the inference from the Snowflake model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from snowflake.snowpark import Session\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "\n",
    "with open('../../creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_WH = data['warehouse']\n",
    "    passphrase = data['passphrase']\n",
    "\n",
    "# Read the private key from the .p8 file\n",
    "with open('../../rsa_key.p8', 'rb') as key_file:\n",
    "    private_key = key_file.read()\n",
    "\n",
    "# If the private key is encrypted, load it with a passphrase\n",
    "# Replace 'your_key_passphrase' with your actual passphrase if needed\n",
    "private_key_obj = serialization.load_pem_private_key(\n",
    "    private_key,\n",
    "    password=passphrase.encode() if passphrase else None,\n",
    "    backend=default_backend()\n",
    ")\n",
    "\n",
    "# Define connection parameters including the private key\n",
    "CONNECTION_PARAMETERS = {\n",
    "    'user': USERNAME,\n",
    "    'account': SF_ACCOUNT,\n",
    "    'private_key': private_key_obj,\n",
    "    'warehouse': SF_WH,\n",
    "}\n",
    "\n",
    "# Create a session with the specified connection parameters\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core import Root\n",
    "root = Root(session)\n",
    "from snowflake.snowpark.functions import col \n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b1e3f-fc07-4598-955b-063bbcb93efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.model import model_signature\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a437716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.version import VERSION\n",
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "from snowflake.ml import version\n",
    "mlversion = version.VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n",
    "print('Snowflake ML version        : {}.{}.{}'.format(mlversion[0],mlversion[2],mlversion[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5a9dd-a700-4de2-8065-4308678fd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY_DATABASE_NAME = \"TPCDS_XGBOOST\"\n",
    "REGISTRY_SCHEMA_NAME = \"DEMO\"\n",
    "reg = registry.Registry(session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593065f",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Series - Takes 2 minutes to run\n",
    "#Only need to run this the first time\n",
    "from statsforecast.utils import generate_series\n",
    "#for length in [10_000, 100_000, 500_000, 1_000_000, 2_000_000]:\n",
    "for length in [5_000]:\n",
    "\t\tprint(f'length: {length}')\n",
    "\t\tseries = generate_series(n_series=length, seed=1)\n",
    "\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd62cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for training\n",
    "df = pd.DataFrame(series)\n",
    "train_df = df.reset_index()\n",
    "train_df.columns = ['ID', 'DS', 'Y']\n",
    "train_df['ID'] = train_df['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save series to Snowflake table\n",
    "#Only need to run this the first time\n",
    "test_df = session.create_dataframe(train_df)\n",
    "test_df.write.mode('overwrite').save_as_table('TPCDS_XGBOOST.DEMO.Series2M')\n",
    "#train_df = session.table('TPCDS_XGBOOST.DEMO.SERIES2M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72106962",
   "metadata": {},
   "source": [
    "## Train Model Locally\n",
    "\n",
    "Assuming you want to do this in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, Naive\n",
    "\n",
    "df = train_df[train_df['ID'] == 2]\n",
    "df.columns = ['unique_id', 'ds', 'y']\n",
    "sf = StatsForecast(df=df,\n",
    "                   models=[AutoARIMA(), Naive()],\n",
    "                   freq='D', \n",
    "                   n_jobs=-1)\n",
    "forecasts_df = sf.forecast(h=7)\n",
    "forecasts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastingModel(custom_model.CustomModel):\n",
    "\n",
    "    # Use the same decorator as for methods with FUNCTION inference.\n",
    "    @custom_model.partitioned_inference_api\n",
    "    def predict(self, df: pd.DataFrame) -> pd.DataFrame:        \n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import AutoARIMA, Naive\n",
    "        df.columns = ['unique_id', 'ds', 'y']\n",
    "        df.head()\n",
    "        model = StatsForecast(models=[AutoARIMA()],\n",
    "                      freq='D',\n",
    "                      n_jobs=-1)  ##chaning to 1 from -1\n",
    "\n",
    "        forecasts_df = model.forecast(df=df, h=7)\n",
    "        forecasts_df.columns = ['DSOUT', 'AUTOARIMA']\n",
    "        return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad073a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model = ForecastingModel()\n",
    "local_predictions = ts_model.predict(train_df[train_df['ID'] == 2])\n",
    "local_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ec46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parallelize\n",
    "def parallel_predict(all_data, model):\n",
    "    return model.predict(all_data)\n",
    "\n",
    "# Assuming df1 is your complete dataset\n",
    "all_groups = [group for _, group in train_df.groupby('ID')]\n",
    "\n",
    "# Initialize the ForecastingModel\n",
    "model = ForecastingModel()\n",
    "\n",
    "# Parallel execution using Joblib\n",
    "num_cores = -1  # Use all available cores\n",
    "results = Parallel(n_jobs=num_cores)(delayed(parallel_predict)(all_data, model) for all_data in all_groups)\n",
    "results\n",
    "# Combine or process the results as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00240d15",
   "metadata": {},
   "source": [
    "## Train Model in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be71cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"function_type\": \"TABLE_FUNCTION\",\n",
    "}\n",
    "\n",
    "mv = reg.log_model(\n",
    "    ts_model,\n",
    "    model_name=\"statsforecast\",\n",
    "    version_name=\"v10\",\n",
    "    conda_dependencies=[\"pandas\", \"statsforecast\"],\n",
    "    options=options,\n",
    "    signatures={\n",
    "        \"predict\": model_signature.ModelSignature(\n",
    "            inputs=[\n",
    "                model_signature.FeatureSpec(name=\"ID\", dtype=model_signature.DataType.INT64),\n",
    "                model_signature.FeatureSpec(name=\"DS\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n",
    "                model_signature.FeatureSpec(name=\"Y\", dtype=model_signature.DataType.DOUBLE),\n",
    "            ],\n",
    "            outputs=[\n",
    "               # model_signature.FeatureSpec(name=\"ID\", dtype=model_signature.DataType.INT64),\n",
    "                model_signature.FeatureSpec(name=\"DSOUT\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n",
    "                model_signature.FeatureSpec(name=\"AUTOARIMA\", dtype=model_signature.DataType.FLOAT),\n",
    "            ],\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = reg.get_model(\"statsforecast\").version(\"v10\")  #v8 is njobs=-1 and v9 is njobs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## running the pandas dataframe in a distributed way for training the models\n",
    "result = reg_model.run(train_df, partition_column=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = session.create_dataframe(train_df)\n",
    "test_df.write.mode('overwrite').save_as_table('TPCDS_XGBOOST.DEMO.TEMPTS')\n",
    "df2 = session.table('TPCDS_XGBOOST.DEMO.TEMPTS')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This runs fast using Snowflake DF on the paritioned model\n",
    "result = reg_model.run(df2, partition_column=\"ID\",function_name=\"PREDICT\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ffd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_opt_wh = Warehouse(\n",
    "  name=\"snowpark_opt_wh\",\n",
    "  warehouse_size=\"MEDIUM\",\n",
    "  warehouse_type = \"SNOWPARK-OPTIMIZED\",\n",
    "  auto_suspend=600,\n",
    ")\n",
    "warehouses = root.warehouses[\"snowpark_opt_wh\"]\n",
    "warehouses.create_or_alter(snowpark_opt_wh)\n",
    "session.use_warehouse(\"snowpark_opt_wh\")\n",
    "\n",
    "session.sql('alter session set USE_CACHED_RESULT = FALSE').collect()\n",
    "session.sql('alter session set query_tag = \"TS-LARGE-Chase\" ').collect()\n",
    "#session.sql('alter warehouse snowpark_opt_wh set max_concurrency_level = 1').collect()\n",
    "\n",
    "print(session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8507f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [10_000, 50_000, 100_000, 500_000, 1_000_000, 2_000_000]\n",
    "lengths = [5_000]\n",
    "\n",
    "train_df = session.table('TPCDS_XGBOOST.DEMO.SERIES2M')\n",
    "\n",
    "for length in lengths:\n",
    "  print (\"prepping data\")\n",
    "  unique_ids_df = train_df.select(\"ID\").distinct().limit(length)\n",
    "  filtered_df = train_df.join(unique_ids_df, on=\"ID\", how=\"inner\").cache_result() #added cache result\n",
    "  print(unique_ids_df.count())\n",
    "  filtered_df.write.mode('overwrite').save_as_table('TPCDS_XGBOOST.DEMO.TEMP_TS')\n",
    "  df2 = session.table('TPCDS_XGBOOST.DEMO.TEMP_TS')\n",
    "  print (\"starting training\")\n",
    "  init = time()\n",
    "  # Run the regression model\n",
    "  result = reg_model.run(df2, partition_column=\"ID\").collect()\n",
    "  total_time = (time() - init) / 60\n",
    "  print(f'n_series: {length} total time: {total_time} total rows: {filtered_df.count()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
