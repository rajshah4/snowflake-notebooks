{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2452b045-2bcc-4df1-bfdf-6888c8320ed5",
   "metadata": {},
   "source": [
    "# Partitioned Custom ML Model with Model Registry\n",
    "\n",
    "This notebook includes two different models and datasets. They are both capable of being tested locally as well as run entirely in Snowflake. I have also made it so you can push the datasets into a Snowflake table for running the inference from the Snowflake model registry.\n",
    "\n",
    "### Partitioned restaurant traffic forecasting model\n",
    "\n",
    "The dataset is loaded locally from the `Partitioned_Custom_Model_Restaurant_Traffic_Data.csv` file.\n",
    "\n",
    "Change `\"MY_DB\"` and `\"MY_SCHEMA\"` to your desired existing database and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from snowflake.snowpark import Session\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "\n",
    "with open('../../creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_WH = data['warehouse']\n",
    "    passphrase = data['passphrase']\n",
    "\n",
    "# Read the private key from the .p8 file\n",
    "with open('../../rsa_key.p8', 'rb') as key_file:\n",
    "    private_key = key_file.read()\n",
    "\n",
    "# If the private key is encrypted, load it with a passphrase\n",
    "# Replace 'your_key_passphrase' with your actual passphrase if needed\n",
    "private_key_obj = serialization.load_pem_private_key(\n",
    "    private_key,\n",
    "    password=passphrase.encode() if passphrase else None,\n",
    "    backend=default_backend()\n",
    ")\n",
    "\n",
    "# Define connection parameters including the private key\n",
    "CONNECTION_PARAMETERS = {\n",
    "    'user': USERNAME,\n",
    "    'account': SF_ACCOUNT,\n",
    "    'private_key': private_key_obj,\n",
    "    'warehouse': SF_WH,\n",
    "}\n",
    "\n",
    "# Create a session with the specified connection parameters\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core import Root\n",
    "root = Root(session)\n",
    "from snowflake.snowpark.functions import col \n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b1e3f-fc07-4598-955b-063bbcb93efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.model import model_signature\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a437716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.version import VERSION\n",
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "from snowflake.ml import version\n",
    "mlversion = version.VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n",
    "print('Snowflake ML version        : {}.{}.{}'.format(mlversion[0],mlversion[2],mlversion[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5a9dd-a700-4de2-8065-4308678fd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY_DATABASE_NAME = \"TPCDS_XGBOOST\"\n",
    "REGISTRY_SCHEMA_NAME = \"DEMO\"\n",
    "reg = registry.Registry(session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78f580-67a0-4cce-b44e-f7c8424f9b43",
   "metadata": {},
   "source": [
    "#### The dataset contains an epoch timestamp in milliseconds, a store ID which will later be used as a partition column, a feature column `COLLEGE_TOWN`, and a target to be forecasted, `HOURLY_TRAFFIC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2387e2-0c09-4a56-893b-85d64e3f2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv file into pandas dataframe.\n",
    "test_df_pandas = pd.read_csv(\"Partitioned_Custom_Model_Restaurant_Traffic_Data.csv\")\n",
    "test_df = session.create_dataframe(test_df_pandas)\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09535059",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.write.mode('overwrite').save_as_table('TPCDS_XGBOOST.DEMO.Restaurant_Traffic_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e284ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = session.table('TPCDS_XGBOOST.DEMO.Restaurant_Traffic_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3e762",
   "metadata": {},
   "source": [
    "Data set is \n",
    "5209585 rows with 200 unique store IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b31eb8-d9c3-4257-993c-4544ddb7a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_store_count = test_df.select(test_df['STORE_ID']).distinct().count()\n",
    "print(unique_store_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7decef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ba1e0-370f-4cc8-86c4-dec4fc9f153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastingModel(custom_model.CustomModel):\n",
    "\n",
    "    # Use the same decorator as for methods with FUNCTION inference.\n",
    "    @custom_model.partitioned_inference_api\n",
    "    def predict(self, df: pd.DataFrame) -> pd.DataFrame:        \n",
    "        import xgboost\n",
    "\n",
    "        # Set the time column as our index.\n",
    "        input_df = df.set_index('EPOCH')\n",
    "        input_df.index = pd.to_datetime(df['EPOCH'], unit='ms')\n",
    "\n",
    "        # Generate categorical features using the datetime index.\n",
    "        input_df['HOUR'] = input_df.index.hour.astype(\"category\")\n",
    "        input_df['DAY_OF_WEEK'] = input_df.index.dayofweek.astype(\"category\")\n",
    "        input_df['MONTH'] = input_df.index.month.astype(\"category\")\n",
    "        input_df['YEAR'] = input_df.index.year.astype(\"category\")\n",
    "        \n",
    "        input_df['COLLEGE_TOWN'] = input_df['COLLEGE_TOWN'].astype(\"category\")\n",
    "        \n",
    "        # Use get_dummies (one-hot encoding) for categorical features.\n",
    "        final = pd.get_dummies(data=input_df, columns=['COLLEGE_TOWN', 'HOUR', 'MONTH', 'YEAR', 'DAY_OF_WEEK'])\n",
    "\n",
    "        # Define the train & forecast split thresholds.\n",
    "        today = pd.to_datetime('2022-10-01')\n",
    "        yesterday = today - timedelta(days=1)\n",
    "        four_weeks = today + timedelta(days=28)\n",
    "        tomorrow = today + timedelta(days=1)\n",
    "\n",
    "        # Train data starts on June 16th 2018 and ends on September 30th.\n",
    "        train = final[(final.index >= pd.to_datetime('16-Jun-2018')) & (final.index <= pd.to_datetime(yesterday))]\n",
    "        \n",
    "        # The forecast starts from October 1st 2022 and goes 4 weeks into the future.\n",
    "        forecast = final[(final.index >= pd.to_datetime(tomorrow)) & (final.index <= pd.to_datetime(four_weeks))]\n",
    "\n",
    "        # Remove the target from the input dataset, and construct target dataset.\n",
    "        X_train = train.drop('HOURLY_TRAFFIC', axis=1)\n",
    "        y_train = train['HOURLY_TRAFFIC']\n",
    "\n",
    "        X_forecast = forecast.drop('HOURLY_TRAFFIC', axis=1)\n",
    "        \n",
    "        # Train an XGBoost regression model.\n",
    "        model = xgboost.XGBRegressor(n_estimators=200, n_jobs=1)\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "        # Predict the hourly forecast for the future dates and make sure no predictions are less than zero.\n",
    "        forecast['PREDICTION'] = model.predict(X_forecast)\n",
    "        forecast['EPOCH_OUT'] = [t.value // 10**9 for t in forecast.index]\n",
    "        forecast = forecast[['EPOCH_OUT', 'PREDICTION']]\n",
    "        forecast = forecast.sort_index()\n",
    "        forecast.loc[forecast['PREDICTION'] < 0, 'PREDICTION'] = 0\n",
    "\n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8534f9-71db-4319-be58-5a9ff3f2fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_forecasting_model = ForecastingModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db5be5-6816-492d-991a-144a32d922ae",
   "metadata": {},
   "source": [
    "#### The predict method can be tested locally by using a pandas dataframe directly. Here we can run `predict` for a single partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b11059-8d8f-4023-a3a8-feaab66bd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_forecasting_model.predict(test_df_pandas.loc[test_df_pandas['STORE_ID'] == 1])\n",
    "#my_forecasting_model.predict(test_df_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad2585-c013-4730-b617-b2336d176479",
   "metadata": {},
   "source": [
    "#### Log the model, specifying the `function_type: \"TABLE_FUNCTION\"` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e77f15-ae88-42fc-b6d3-e21665cd6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"function_type\": \"TABLE_FUNCTION\",\n",
    "}\n",
    "\n",
    "mv = reg.log_model(\n",
    "    my_forecasting_model,\n",
    "    model_name=\"forecast\",\n",
    "    version_name=\"v13\",\n",
    "    conda_dependencies=[\"pandas\", \"scikit-learn\", \"xgboost\"],\n",
    "    options=options,\n",
    "    signatures={\n",
    "        \"predict\": model_signature.ModelSignature(\n",
    "            inputs=[\n",
    "                model_signature.FeatureSpec(name=\"EPOCH\", dtype=model_signature.DataType.DOUBLE),\n",
    "                model_signature.FeatureSpec(name=\"STORE_ID\", dtype=model_signature.DataType.DOUBLE),\n",
    "                model_signature.FeatureSpec(name=\"COLLEGE_TOWN\", dtype=model_signature.DataType.DOUBLE),\n",
    "                model_signature.FeatureSpec(name=\"HOURLY_TRAFFIC\", dtype=model_signature.DataType.INT64),\n",
    "            ],\n",
    "            outputs=[\n",
    "                model_signature.FeatureSpec(name=\"EPOCH_OUT\", dtype=model_signature.DataType.FLOAT),\n",
    "                model_signature.FeatureSpec(name=\"PREDICTION\", dtype=model_signature.DataType.FLOAT),\n",
    "            ],\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fcde38-5b9d-4b96-8833-f699611c643b",
   "metadata": {},
   "source": [
    "#### Use the `run` method for inference, specifying the partition column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a970ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = reg.get_model(\"forecast\").version(\"v13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_opt_wh = Warehouse(\n",
    "  name=\"snowpark_opt_wh\",\n",
    "  warehouse_size=\"LARGE\",\n",
    "  warehouse_type = \"SNOWPARK-OPTIMIZED\",\n",
    "  auto_suspend=600,\n",
    ")\n",
    "warehouses = root.warehouses[\"snowpark_opt_wh\"]\n",
    "warehouses.create_or_alter(snowpark_opt_wh)\n",
    "\n",
    "session.sql('USE WAREHOUSE SNOWPARK_OPT_WH').collect()\n",
    "session.sql('alter session set USE_CACHED_RESULT = FALSE').collect()\n",
    "session.sql('alter session set query_tag = \"TS_XG_LARGE\" ').collect()\n",
    "print(session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dda197-3afd-47a3-9fd6-d1237af6804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mv.run(test_df, partition_column=\"STORE_ID\")\n",
    "result.select(\"EPOCH_OUT\", \"PREDICTION\", \"STORE_ID\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimized S it ran in 24 seconds \n",
    "## Optimized M is ran in 21 seconds \n",
    "\n",
    "\n",
    "Raj test:\n",
    "## Optimized L ran in 15 seconds\n",
    "\n",
    "## Local test - one thread it ran in 1 minute 40 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593065f",
   "metadata": {},
   "source": [
    "### StatsForecast Arima Model on Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Series - Takes 2 minutes to run\n",
    "#Only need to run this the first time\n",
    "from statsforecast.utils import generate_series\n",
    "for length in [10_000, 100_000, 500_000, 1_000_000, 2_000_000]:\n",
    "\t\tprint(f'length: {length}')\n",
    "\t\tseries = generate_series(n_series=length, seed=1)\n",
    "\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save series to Snowflake table\n",
    "#Only need to run this the first time\n",
    "df = pd.DataFrame(series)\n",
    "df_reset = df.reset_index()\n",
    "df_reset.columns = ['ID', 'DS', 'Y']\n",
    "\n",
    "test_df = session.create_dataframe(df_reset)\n",
    "test_df.write.mode('overwrite').save_as_table('TPCDS_XGBOOST.DEMO.Series2M')\n",
    "train_df = session.table('TPCDS_XGBOOST.DEMO.SERIES2M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve from Snowflake -- \n",
    "train_df = session.table('TPCDS_XGBOOST.DEMO.SERIES2M')\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Local Test for Model\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, Naive\n",
    "\n",
    "model = StatsForecast(models=[AutoARIMA(), Naive()],\n",
    "                      freq='D',\n",
    "                      n_jobs=-1)\n",
    "df_reset.columns = ['unique_id', 'ds', 'y']\n",
    "forecasts_df = model.forecast(df=df_reset, h=7)\n",
    "forecasts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastingModel(custom_model.CustomModel):\n",
    "\n",
    "    # Use the same decorator as for methods with FUNCTION inference.\n",
    "    @custom_model.partitioned_inference_api\n",
    "    def predict(self, df: pd.DataFrame) -> pd.DataFrame:        \n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import AutoARIMA, Naive\n",
    "        df.columns = ['unique_id', 'ds', 'y']\n",
    "        model = StatsForecast(models=[AutoARIMA()],\n",
    "                      freq='D',\n",
    "                      n_jobs=-1)\n",
    "\n",
    "        forecasts_df = model.forecast(df=df, h=7)\n",
    "        forecasts_df.columns = ['DSOUT', 'AUTOARIMA']\n",
    "\n",
    "        return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_forecasting_model = ForecastingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_forecasting_model.predict(df_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"function_type\": \"TABLE_FUNCTION\",\n",
    "}\n",
    "\n",
    "mv = reg.log_model(\n",
    "    my_forecasting_model,\n",
    "    model_name=\"statsforecast\",\n",
    "    version_name=\"v8\",\n",
    "    conda_dependencies=[\"pandas\", \"statsforecast\"],\n",
    "    options=options,\n",
    "    signatures={\n",
    "        \"predict\": model_signature.ModelSignature(\n",
    "            inputs=[\n",
    "                model_signature.FeatureSpec(name=\"ID\", dtype=model_signature.DataType.INT64),\n",
    "                model_signature.FeatureSpec(name=\"DS\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n",
    "                model_signature.FeatureSpec(name=\"Y\", dtype=model_signature.DataType.DOUBLE),\n",
    "            ],\n",
    "            outputs=[\n",
    "               # model_signature.FeatureSpec(name=\"ID\", dtype=model_signature.DataType.INT64),\n",
    "                model_signature.FeatureSpec(name=\"DSOUT\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n",
    "                model_signature.FeatureSpec(name=\"AUTOARIMA\", dtype=model_signature.DataType.FLOAT),\n",
    "            ],\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = reg.get_model(\"statsforecast\").version(\"v8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_opt_wh = Warehouse(\n",
    "  name=\"snowpark_opt_wh\",\n",
    "  warehouse_size=\"LARGE\",\n",
    "  #warehouse_type = \"SNOWPARK-OPTIMIZED\",\n",
    "  auto_suspend=600,\n",
    ")\n",
    "warehouses = root.warehouses[\"snowpark_opt_wh\"]\n",
    "warehouses.create_or_alter(snowpark_opt_wh)\n",
    "session.use_warehouse(\"snowpark_opt_wh\")\n",
    "\n",
    "session.sql('alter session set USE_CACHED_RESULT = FALSE').collect()\n",
    "session.sql('alter session set query_tag = \"TS-LARGE-Chase\" ').collect()\n",
    "#session.sql('alter warehouse snowpark_opt_wh set max_concurrency_level = 1').collect()\n",
    "\n",
    "print(session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8507f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [10_000, 50_000, 100_000, 500_000, 1_000_000,2_000_000]\n",
    "#lengths = [10_000]\n",
    "\n",
    "for length in lengths:\n",
    "  unique_ids_df = train_df.select(\"ID\").distinct().limit(length)\n",
    "  filtered_df = train_df.join(unique_ids_df, on=\"ID\", how=\"inner\").cache_result() #added cache result\n",
    "  print(unique_ids_df.count())\n",
    "  init = time()\n",
    "  # Run the regression model\n",
    "  result = reg_model.run(filtered_df, partition_column=\"ID\").collect()\n",
    "  total_time = (time() - init) / 60\n",
    "  print(f'n_series: {length} total time: {total_time} total rows: {filtered_df.count()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
