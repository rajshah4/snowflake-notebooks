{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2452b045-2bcc-4df1-bfdf-6888c8320ed5",
   "metadata": {},
   "source": [
    "# Partitioned Custom Time Series Model - Statsmodels Exponential Smoothing\n",
    "\n",
    "This notebook shows how to partition a time series model. It uses synthetic data and statsmodels exponetial smoothing. The notebooks shows how to test it locally and then run the model in a distributed fashion in Snowflake. I have also made it so you can push the datasets into a Snowflake table for running the inference from the Snowflake model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from snowflake.snowpark import Session\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "\n",
    "with open('../../creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_WH = data['warehouse']\n",
    "    passphrase = data['passphrase']\n",
    "\n",
    "# Read the private key from the .p8 file\n",
    "with open('../../rsa_key.p8', 'rb') as key_file:\n",
    "    private_key = key_file.read()\n",
    "\n",
    "# If the private key is encrypted, load it with a passphrase\n",
    "# Replace 'your_key_passphrase' with your actual passphrase if needed\n",
    "private_key_obj = serialization.load_pem_private_key(\n",
    "    private_key,\n",
    "    password=passphrase.encode() if passphrase else None,\n",
    "    backend=default_backend()\n",
    ")\n",
    "\n",
    "# Define connection parameters including the private key\n",
    "CONNECTION_PARAMETERS = {\n",
    "    'user': USERNAME,\n",
    "    'account': SF_ACCOUNT,\n",
    "    'private_key': private_key_obj,\n",
    "    'warehouse': SF_WH,\n",
    "}\n",
    "\n",
    "# Create a session with the specified connection parameters\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core import Root\n",
    "root = Root(session)\n",
    "from snowflake.snowpark.functions import col \n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b1e3f-fc07-4598-955b-063bbcb93efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.model import model_signature\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a437716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.version import VERSION\n",
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "from snowflake.ml import version\n",
    "mlversion = version.VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n",
    "print('Snowflake ML version        : {}.{}.{}'.format(mlversion[0],mlversion[2],mlversion[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5a9dd-a700-4de2-8065-4308678fd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY_DATABASE_NAME = \"TPCDS_XGBOOST\"\n",
    "REGISTRY_SCHEMA_NAME = \"DEMO\"\n",
    "reg = registry.Registry(session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593065f",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Series - Takes 2 minutes to run\n",
    "#Only need to run this the first time\n",
    "from statsforecast.utils import generate_series\n",
    "#for length in [10_000, 100_000, 500_000, 1_000_000, 2_000_000]:\n",
    "for length in [500_000]:\n",
    "\t\tprint(f'length: {length}')\n",
    "\t\tseries = generate_series(n_series=length, seed=1)\n",
    "\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd62cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for training\n",
    "df = pd.DataFrame(series)\n",
    "train_df = df.reset_index()\n",
    "train_df.columns = ['ID', 'DS', 'Y']\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72106962",
   "metadata": {},
   "source": [
    "## Train Model Locally\n",
    "\n",
    "you want to use pandas for initial local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test for Exponential Smoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "df = train_df[train_df['ID'] == 1]\n",
    "df.set_index('DS', inplace=True)\n",
    "model = ExponentialSmoothing(df['Y'], seasonal=None, trend='add', damped_trend=False)\n",
    "fit = model.fit()\n",
    "forecast = fit.forecast(steps=6)\n",
    "forecast_df = pd.DataFrame({\n",
    "                'DATE': forecast.index,\n",
    "                'FORECAST': forecast.values\n",
    "            })\n",
    "forecast_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ES Model\n",
    "class ForecastingModel(custom_model.CustomModel):\n",
    "    # Use the same decorator as for methods with FUNCTION inference.\n",
    "    @custom_model.partitioned_inference_api\n",
    "    def predict(self, df:pd.DataFrame) -> pd.DataFrame:    #Please keep input and output here as pandas   \n",
    "        ################## Replace below with your python code ######################################## \n",
    "        import pandas as pd\n",
    "        from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "        from datetime import datetime, timedelta\n",
    "        df.set_index('DS', inplace=True)\n",
    "        df = df.asfreq('D') \n",
    "        model = ExponentialSmoothing(df['Y'], seasonal=None, trend='add', damped_trend=False,freq='D')\n",
    "        fit = model.fit()\n",
    "        forecast = fit.forecast(steps=7)\n",
    "        forecast_df = pd.DataFrame({\n",
    "                        'DATE': forecast.index,\n",
    "                        'FORECAST': forecast.values\n",
    "                    })\n",
    "        return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad073a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_model = ForecastingModel()\n",
    "local_predictions = es_model.predict(train_df[train_df['ID'] == 1])\n",
    "#local_predictions = es_model.predict(train_df)\n",
    "local_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ec46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parallelize\n",
    "def parallel_predict(all_data, model):\n",
    "    return model.predict(all_data)\n",
    "\n",
    "# Assuming df1 is your complete dataset\n",
    "all_groups = [group for _, group in train_df.groupby('ID')]\n",
    "\n",
    "# Initialize the ForecastingModel\n",
    "model = ForecastingModel()\n",
    "\n",
    "# Parallel execution using Joblib\n",
    "num_cores = -1  # Use all available cores\n",
    "results = Parallel(n_jobs=num_cores)(delayed(parallel_predict)(all_data, model) for all_data in all_groups)\n",
    "\n",
    "# Combine or process the results as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00240d15",
   "metadata": {},
   "source": [
    "## Train Model in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be71cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"function_type\": \"TABLE_FUNCTION\",\n",
    "}\n",
    "mv = reg.log_model(\n",
    "    es_model,\n",
    "    model_name=\"es_forecast\",\n",
    "    version_name=\"v9\",\n",
    "    conda_dependencies=['pandas', 'statsmodels', 'snowflake-snowpark-python'],\n",
    "    options=options,\n",
    "    signatures={\n",
    "        \"predict\": model_signature.ModelSignature(\n",
    "            inputs=[\n",
    "                model_signature.FeatureSpec(name=\"ID\", dtype=model_signature.DataType.INT64),\n",
    "                model_signature.FeatureSpec(name=\"DS\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n",
    "                model_signature.FeatureSpec(name=\"Y\", dtype=model_signature.DataType.FLOAT),\n",
    "            ],\n",
    "            outputs=[\n",
    "                model_signature.FeatureSpec(name=\"DSOUT\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n",
    "                model_signature.FeatureSpec(name=\"FORECAST\", dtype=model_signature.DataType.FLOAT),\n",
    "            ],\n",
    "         )\n",
    "     },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = reg.get_model(\"es_forecast\").version(\"v9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## running the pandas dataframe in a distributed way for training the models\n",
    "result = reg_model.run(train_df, partition_column=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24573958",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f26a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's get a snowpark dataframe \n",
    "test_df = session.create_dataframe(train_df)\n",
    "test_df.write.mode('overwrite').save_as_table('TPCDS_XGBOOST.DEMO.TEMPTS')\n",
    "df2 = session.table('TPCDS_XGBOOST.DEMO.TEMPTS')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run the model across the Snowflake DF. It should be faster than the pandas dataframe - especially at scale.\n",
    "result = reg_model.run(df2, partition_column=\"ID\",function_name=\"PREDICT\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_opt_wh = Warehouse(\n",
    "  name=\"snowpark_opt_wh\",\n",
    "  warehouse_size=\"MEDIUM\",\n",
    "  warehouse_type = \"SNOWPARK-OPTIMIZED\",\n",
    "  auto_suspend=600,\n",
    ")\n",
    "warehouses = root.warehouses[\"snowpark_opt_wh\"]\n",
    "warehouses.create_or_alter(snowpark_opt_wh)\n",
    "session.use_warehouse(\"snowpark_opt_wh\")\n",
    "\n",
    "session.sql('alter session set USE_CACHED_RESULT = FALSE').collect()\n",
    "session.sql('alter session set query_tag = \"TS-LARGE-Chase\" ').collect()\n",
    "#session.sql('alter warehouse snowpark_opt_wh set max_concurrency_level = 1').collect()\n",
    "\n",
    "print(session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa8507f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_series: 2000000 total time: 33.24094546238582 total rows: 549884998\n"
     ]
    }
   ],
   "source": [
    "lengths = [10_000, 50_000, 100_000, 500_000, 1_000_000,2_000_000]\n",
    "#lengths = [5_000]\n",
    "\n",
    "train_df = session.table('TPCDS_XGBOOST.DEMO.SERIES2M')\n",
    "session.sql('ALTER TABLE TPCDS_XGBOOST.DEMO.SERIES2M CLUSTER BY (ID);').collect()\n",
    "\n",
    "\n",
    "for length in lengths:\n",
    "  print (\"prepping data\")\n",
    "  df2 = train_df.filter((col(\"ID\") >= 0) & (col(\"ID\") <= (length-1)))\n",
    "  print (\"starting training\")\n",
    "  init = time()\n",
    "  # Run the regression model\n",
    "  result = reg_model.run(df2, partition_column=\"ID\").collect()\n",
    "  total_time = (time() - init) / 60\n",
    "  print(f'n_series: {length} total time: {total_time} total rows: {filtered_df.count()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
