{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2452b045-2bcc-4df1-bfdf-6888c8320ed5",
   "metadata": {},
   "source": [
    "# Partitioned Custom Time Series Model - Nixta AutoArima\n",
    "\n",
    "This notebook shows how to partition a time series model. It uses synthetic data and Nixta AutoArima. The notebooks shows how to test it locally and then run the model in a distributed fashion in Snowflake. I have also made it so you can push the datasets into a Snowflake table for running the inference from the Snowflake model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a2c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from snowflake.snowpark import Session\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "\n",
    "with open('../../creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_WH = data['warehouse']\n",
    "    passphrase = data['passphrase']\n",
    "\n",
    "# Read the private key from the .p8 file\n",
    "with open('../../rsa_key.p8', 'rb') as key_file:\n",
    "    private_key = key_file.read()\n",
    "\n",
    "# If the private key is encrypted, load it with a passphrase\n",
    "# Replace 'your_key_passphrase' with your actual passphrase if needed\n",
    "private_key_obj = serialization.load_pem_private_key(\n",
    "    private_key,\n",
    "    password=passphrase.encode() if passphrase else None,\n",
    "    backend=default_backend()\n",
    ")\n",
    "\n",
    "# Define connection parameters including the private key\n",
    "CONNECTION_PARAMETERS = {\n",
    "    'user': USERNAME,\n",
    "    'account': SF_ACCOUNT,\n",
    "    'private_key': private_key_obj,\n",
    "    'warehouse': SF_WH,\n",
    "}\n",
    "\n",
    "# Create a session with the specified connection parameters\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core import Root\n",
    "root = Root(session)\n",
    "from snowflake.snowpark.functions import col \n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3b1e3f-fc07-4598-955b-063bbcb93efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.model import model_signature\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffe9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "import pandas as pd\n",
    "from absl.testing import absltest, parameterized\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from snowflake import snowpark\n",
    "from snowflake.ml import registry\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.model.model_signature import infer_signature\n",
    "from snowflake.ml.utils import connection_params\n",
    "from snowflake.snowpark import functions as F\n",
    "#from tests.integ.snowflake.ml.test_utils import db_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a437716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                        : RSHAH\n",
      "Role                        : \"RAJIV\"\n",
      "Database                    : \"RAJIV\"\n",
      "Schema                      : \"PUBLIC\"\n",
      "Warehouse                   : \"RAJIV\"\n",
      "Snowflake version           : 8.35.1\n",
      "Snowpark for Python version : 1.20.0\n",
      "Snowflake ML version        : 1.6.1\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark.version import VERSION\n",
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "from snowflake.ml import version\n",
    "mlversion = version.VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n",
    "print('Snowflake ML version        : {}.{}.{}'.format(mlversion[0],mlversion[2],mlversion[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4d5a9dd-a700-4de2-8065-4308678fd917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Table RESULTS successfully created.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATABASE_NAME = \"TPCDS_XGBOOST\"\n",
    "SCHEMA_NAME = \"DEMO\"\n",
    "reg = registry.Registry(session=session, database_name=DATABASE_NAME, schema_name=SCHEMA_NAME)\n",
    "\n",
    "reg_model = reg.get_model(\"statsforecast\").version(\"v12\") \n",
    "\n",
    "session.sql('USE TPCDS_XGBOOST.DEMO').collect()\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS RESULTS (\n",
    "    test_id NUMBER,\n",
    "    model VARCHAR,\n",
    "    query_id VARCHAR,\n",
    "    nrows VARCHAR,\n",
    "    time VARCHAR,\n",
    "    dataset_name VARCHAR,\n",
    "    warehouse_size VARCHAR,\n",
    "    warehouse_type VARCHAR\n",
    ")\n",
    "\"\"\"\n",
    "session.sql(query).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed0d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72106962",
   "metadata": {},
   "source": [
    "## Train Model Locally\n",
    "\n",
    "You want to use pandas for initial local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a9f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ID = 5\n",
    "\n",
    "WAREHOUSE_SIZES = [\n",
    "    # \"XSMALL\",\n",
    "    \"SMALL\",\n",
    "    \"MEDIUM\",\n",
    "    \"LARGE\",\n",
    "  #  \"XLARGE\",\n",
    "  #  \"XXLARGE\",\n",
    "]\n",
    "\n",
    "WAREHOUSE_TYPES = [\n",
    "    \"STANDARD\",\n",
    "    # \"SNOWPARK-OPTIMIZED\"\n",
    "]\n",
    "\n",
    "DATASETS = [\n",
    "    (1_000_000, 10),\n",
    "    #(10_000_000, 10),\n",
    "    # (10_000_000, 100),\n",
    "   # (100_000_000, 10),\n",
    "    # (100_000_000, 100),\n",
    "  #  (1_000_000_000, 10),\n",
    "    # (1_000_000_000, 100),\n",
    "    # (10_000_000_000, 10),\n",
    "]  # rows x features\n",
    "\n",
    "REPEAT_TIMES = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa8507f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"SNOWPARK_OPT_WH\"\n",
      "build warehouse  LARGE\n",
      "prepping data for  10000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 10000 total time: 2.7434086481730144 total rows: 2750459\n",
      "prepping data for  50000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 50000 total time: 9.841282288233439 total rows: 13776842\n",
      "prepping data for  100000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 100000 total time: 18.440611879030865 total rows: 27514605\n",
      "prepping data for  500000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 500000 total time: 88.59080203771592 total rows: 137488809\n",
      "prepping data for  1000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 1000000 total time: 175.46309643189113 total rows: 274926721\n",
      "prepping data for  2000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 2000000 total time: 349.8397043506304 total rows: 549884998\n",
      "build warehouse  XXLARGE\n",
      "prepping data for  10000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 10000 total time: 1.3042607665061952 total rows: 2750459\n",
      "prepping data for  50000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 50000 total time: 2.9267533977826434 total rows: 13776842\n",
      "prepping data for  100000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 100000 total time: 5.334554119904836 total rows: 27514605\n",
      "prepping data for  500000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 500000 total time: 23.928463804721833 total rows: 137488809\n",
      "prepping data for  1000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 1000000 total time: 46.567475481828055 total rows: 274926721\n",
      "prepping data for  2000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 2000000 total time: 91.02804658412933 total rows: 549884998\n",
      "build warehouse  XXXLARGE\n",
      "prepping data for  10000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 10000 total time: 1.1376259843508403 total rows: 2750459\n",
      "prepping data for  50000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 50000 total time: 1.7629268646240235 total rows: 13776842\n",
      "prepping data for  100000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 100000 total time: 2.9738175829251605 total rows: 27514605\n",
      "prepping data for  500000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 500000 total time: 12.927256015936534 total rows: 137488809\n",
      "prepping data for  1000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 1000000 total time: 24.308334096272787 total rows: 274926721\n",
      "prepping data for  2000000\n",
      "\"SNOWPARK_OPT_WH\"\n",
      "starting training\n",
      "n_series: 2000000 total time: 47.65634653568268 total rows: 549884998\n"
     ]
    }
   ],
   "source": [
    "# An evaluation loop to see how the model does at the different series lengths\n",
    "lengths = [10_000, 50_000, 100_000, 500_000, 1_000_000, 2_000_000]\n",
    "#lengths = [1_000]\n",
    "TEST_ID = 1\n",
    "train_df = session.table('TPCDS_XGBOOST.DEMO.SERIES2M')\n",
    "print(session.get_current_warehouse())\n",
    "model_name = \"statsforecast\"\n",
    "dataset_name = \"SERIES2M\"\n",
    "\n",
    "WAREHOUSE_SIZES = [\n",
    "    # \"XSMALL\",\n",
    "   # \"SMALL\",\n",
    "   # \"MEDIUM\",\n",
    "    \"LARGE\",\n",
    "    #\"XLARGE\",\n",
    "    \"XXLARGE\",\n",
    "    \"XXXLARGE\"\n",
    "\n",
    "]\n",
    "for whouse in WAREHOUSE_SIZES:  \n",
    "  print (\"build warehouse \", whouse)\n",
    "  wh_size = whouse\n",
    "  wh_type = \"SNOWPARK-OPTIMIZED\"\n",
    "  wh_type = \"STANDARD\"\n",
    "\n",
    "  session.sql(\n",
    "  f\"\"\"CREATE OR REPLACE WAREHOUSE {\"snowpark_opt_wh\"}\n",
    "          WITH\n",
    "              WAREHOUSE_SIZE= '{wh_size}'\n",
    "              WAREHOUSE_TYPE = '{wh_type}'\n",
    "              AUTO_SUSPEND = 60\n",
    "              AUTO_RESUME = TRUE\n",
    "              INITIALLY_SUSPENDED = FALSE\n",
    "              MAX_CONCURRENCY_LEVEL = 1\n",
    "              MIN_CLUSTER_COUNT = 1\n",
    "              MAX_CLUSTER_COUNT = 1\n",
    "  \"\"\"\n",
    "  ).collect()\n",
    "\n",
    "  for length in lengths:\n",
    "    print (\"prepping data for \", length)\n",
    "    print(session.get_current_warehouse())\n",
    "    df2 = train_df.filter((col(\"ID\") >= 0) & (col(\"ID\") <= (length-1)))\n",
    "    print (\"starting training\")\n",
    "    init = time()\n",
    "    # Run the regression model\n",
    "    result = reg_model.run(df2, partition_column=\"ID\").collect()\n",
    "    total_time = (time() - init) / 60\n",
    "    print(f'n_series: {length} total time: {total_time} total rows: {df2.count()}')\n",
    "    query_id = session.sql(\"SELECT LAST_QUERY_ID()\").collect()[0].as_dict()[\"LAST_QUERY_ID()\"]\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO RESULTS VALUES (\n",
    "      '{TEST_ID}',\n",
    "      '{model_name}',\n",
    "      '{query_id}',\n",
    "      '{length}',\n",
    "      '{total_time}',\n",
    "      '{dataset_name}',\n",
    "      '{wh_size}',\n",
    "      '{wh_type}'\n",
    "      )\n",
    "    \"\"\"\n",
    "    session.sql(query).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ebe1e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"TEST_ID\"  |\"MODEL\"        |\"QUERY_ID\"                            |\"NROWS\"  |\"DATASET_NAME\"  |\"WAREHOUSE_SIZE\"  |\"WAREHOUSE_TYPE\"    |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "|0          |statsforecast  |01b73400-0003-03dc-0023-fc8702e20cea  |1000     |SERIES2M        |MEDIUM            |SNOWPARK-OPTIMIZED  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = session.table('RESULTS')\n",
    "results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
