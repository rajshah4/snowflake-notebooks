{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc587a1-80ad-4e5f-8631-1d9bc25dbbec",
   "metadata": {
    "collapsed": false,
    "name": "intro_notebook"
   },
   "source": [
    "# Run LLAMA3.2-11B-Vision-Instruct on ML Container runtime using GPU_NV_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb963d-f2a6-4706-9487-3ccb56908992",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "Let's use the Llama-3.2-11B-Vision-Instruct and Llama-3.2-90B-Vision-Instruct models from Meta with Hugging Face transformers!  \n",
    "\n",
    "- Turn an image into test\n",
    "- Turn an image of a table into a JSON representation.\n",
    "\n",
    "These multimodal models are capable of visual understanding. They take both text and images as input, check out the model cards for more information [Llama 3.2 11B](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) and [Llama 3.2 90B](https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct) model cards.\n",
    "\n",
    "Meta Llama 3.2 is licensed under the LLAMA 3.2 Community License, Copyright Â© Meta Platforms, Inc. All Rights Reserved. Customers are responsible for ensuring their compliance with the terms of this license and the Llama 3.2 Acceptable Use Policy.\n",
    "\n",
    "Note: Meta does not grant rights for the multimodal models in the Llama 3.2 license to users domiciled in the European Union, or companies with principle place of business in the European Union. See the Llama 3.2 Acceptable Use Policy for more informati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b9479-e00e-4f12-8062-cb9a4fd1cf9e",
   "metadata": {
    "collapsed": false,
    "name": "intro_standard_imports"
   },
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "standard_imports"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98fdc8-192a-47ee-b12a-da7f42b29204",
   "metadata": {
    "collapsed": false,
    "name": "intro_upgrade_transformers"
   },
   "source": [
    "### Upgrade transformers\n",
    "If you get an error here, make sure you have an external integration to install packages. You will also need the external integration for connecting to the Hugging Face hub to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed9318-68c7-4449-a17a-f6ff0f074135",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers>=4.45.0\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a942101-5cfb-4199-a3fb-284b920d44c7",
   "metadata": {
    "collapsed": false,
    "name": "intro_import_task_libs"
   },
   "source": [
    "### Import task specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "import_task_libs"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd33259-39a4-4ab4-973e-ee7c0f45b3ca",
   "metadata": {
    "collapsed": false,
    "name": "intro_set_params"
   },
   "source": [
    "### Set params and log-in into Huggingface hub\n",
    "\n",
    "Add your [Hugging Face token](https://huggingface.co/settings/tokens) for downloading Llama. Please note, the [Llama vision models](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf) are gated. Meta requires you submit a form on Hugging Face for access to the model. You will need to do that before you can download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb7d32-8d6d-48cc-81e5-da8e77734b05",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "set_params"
   },
   "outputs": [],
   "source": [
    "hf_token = \"hf_XXX\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cdb1d9-2547-4e59-8414-0c79582ed56e",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "Let's use transformers which makes it easy to use the new Llama models. The `device_map` option set to `auto` means for multi-GPU systems, the model will automatically use [Big Model Inference](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference) from Hugging Face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "download_model"
   },
   "outputs": [],
   "source": [
    "login(hf_token)\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a616df2-4dda-431b-8e20-d1fffa4af219",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "### Image to Text -- Let's use this image and convert it to text.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:800/format:webp/1*3BMNlDaKPOlijIX-1erbJw.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11765634-9e48-4171-a417-7424bfd6d615",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "open_image"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "url = \"https://miro.medium.com/v2/resize:fit:800/format:webp/1*3BMNlDaKPOlijIX-1erbJw.jpeg\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"If I had to write a haiku for this one, it would be: \"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e94137-835d-4f75-8145-8779dc812261",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "transform_input"
   },
   "outputs": [],
   "source": [
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(image, input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=300)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3cf19-31fb-4857-b591-5a1ec7fff2bd",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "### Table to Text/JSON\n",
    "\n",
    "![image](https://mrkremerscience.com/wp-content/uploads/2013/08/data-table-example1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e641282-5638-4b55-84c9-5a887c724683",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "url = \"https://mrkremerscience.com/wp-content/uploads/2013/08/data-table-example1.png\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Parse the table into a JSON representation where the methods are keys and the datasets are subkeys. \"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(image, input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=300)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7c8d5-dee2-4797-ae0e-d05024a5c7cb",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "### Document Understanding\n",
    "\n",
    "![image](https://huggingface.co/spaces/huggingface-projects/llama-3.2-vision-11B/resolve/main/examples/invoice.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4ae41-0ab9-4a50-b9d0-412bfe181006",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/spaces/huggingface-projects/llama-3.2-vision-11B/resolve/main/examples/invoice.png\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"How long does it take from invoice date to due date? Be short and concise.\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(image, input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=300)\n",
    "print(processor.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
