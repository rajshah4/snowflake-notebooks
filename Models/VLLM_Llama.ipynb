{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba2def-c317-4a3b-9cc3-cae4da6247ef",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174a62f-5bd1-4092-9843-9be8531016ae",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "!pip install vllm==0.6.1 huggingface_hub[cli] mistral_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15856500-a5a3-43ea-99ac-47759943da8a",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login (token=\"hf_XXX\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71747148-e2b4-4fb9-bf34-587c8302e997",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the model\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm = LLM(model=model_name)\n",
    "\n",
    "# Set up sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=100)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt):\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    return outputs[0].outputs[0].text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain the concept of artificial intelligence in simple terms.\"\n",
    "response = generate_text(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3ce54-2bf3-4816-97fb-e1b2369e6b22",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "prompt = \"Explain the concept of artificial intelligence in simple terms.\"\n",
    "response = generate_text(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7d31e-d521-4a79-a3bd-cae4c922f047",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "prompt = \"Explain the concept of semantic search in simple terms.\"\n",
    "response = generate_text(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
