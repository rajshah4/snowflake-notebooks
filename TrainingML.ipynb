{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are several examples of how to use Snowpark ML to train a model in Snowflake.\n",
    "They include:\n",
    "    - Snowflake ML with XGBoost\n",
    "    - Snowflake ML with SciKit\n",
    "    - Stored Procedure for Training a Model with Scikit\n",
    "    - Stored Procedures for Training a Model with Pytorch\n",
    "\n",
    "One of the biggest source of errors is versioning. If you are using Snowflake warehouses, make sure your local environment matches the version of the warehouse. Similarly, when you define stored procedures you can specify the version of packages you are using. Check out the custom packages in the stored procedures for more information. https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.Session.custom_package_usage_config#snowflake.snowpark.Session.custom_package_usage_config\n",
    "\n",
    "\n",
    "Resources:\n",
    "- Snowflake ML Model Docs: https://docs.snowflake.com/en/developer-guide/snowpark-ml/modeling (suppoorts XGBoost, SciKit, and LightGBM) \n",
    "- Snowpark ML API: https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/index- \n",
    "- ML Quickstart: https://quickstarts.snowflake.com/guide/intro_to_machine_learning_with_snowpark_ml_for_python/ \n",
    "- Rajiv's Snowflake Notebooks: https://github.com/rajshah4/snowflake-notebooks\n",
    "\n",
    "\n",
    "Tips: \n",
    "- Use Optimized Warehouses: https://docs.snowflake.com/en/user-guide/warehouses-snowpark-optimized\n",
    "- You can use the MAX_CONCURRENCY_LEVEL parameter to limit the number of concurrent queries running in a warehouse.\n",
    "- Container services for GPUs and support any python package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import Variant\n",
    "from snowflake.snowpark.version import VERSION\n",
    "\n",
    "# Snowpark ML\n",
    "# Misc\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging \n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "from snowflake import connector\n",
    "from snowflake.ml.utils import connection_params\n",
    "\n",
    "from snowflake.ml import version\n",
    "SOURCE = \"SnowML\"\n",
    "MLVersion = version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Secure Connection to Snowflake\n",
    "\n",
    "Using the Snowpark Python API, itâ€™s quick and easy to establish a secure connection between Snowflake and Notebook.\n",
    " *Note: Other connection options include Username/Password, MFA, OAuth, Okta, SSO*\n",
    "\n",
    "I like to store my credentials in creds.json so they aren't in the notebook.\n",
    "The file should look like this:\n",
    "```\n",
    "{\n",
    "    \"account\": \"awb99999\",\n",
    "    \"user\": \"your_user_name\",\n",
    "    \"password\": \"your_password\",\n",
    "    \"warehouse\": \"your_warehouse\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "\n",
    "with open('../creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_WH = data['warehouse']\n",
    "    passphrase = data['passphrase']\n",
    "\n",
    "# Read the private key from the .p8 file\n",
    "with open('../rsa_key.p8', 'rb') as key_file:\n",
    "    private_key = key_file.read()\n",
    "\n",
    "# If the private key is encrypted, load it with a passphrase\n",
    "# Replace 'your_key_passphrase' with your actual passphrase if needed\n",
    "private_key_obj = serialization.load_pem_private_key(\n",
    "    private_key,\n",
    "    password=passphrase.encode() if passphrase else None,\n",
    "    backend=default_backend()\n",
    ")\n",
    "\n",
    "# Define connection parameters including the private key\n",
    "CONNECTION_PARAMETERS = {\n",
    "    'user': USERNAME,\n",
    "    'account': SF_ACCOUNT,\n",
    "    'private_key': private_key_obj,\n",
    "    'warehouse': SF_WH,\n",
    "   # 'password': PASSWORD,\n",
    "}\n",
    "\n",
    "# Create a session with the specified connection parameters\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                      : RSHAH\n",
      "Role                      : \"RAJIV\"\n",
      "Database                  : \"RAJIV\"\n",
      "Schema                    : \"PUBLIC\"\n",
      "Warehouse                 : \"RAJIV\"\n",
      "Snowflake version         : 8.27.1\n",
      "Snowpark-snowpark-python  : 1.15.0\n",
      "Snowflake-ml-python       : 1.5.3\n"
     ]
    }
   ],
   "source": [
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                      : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                      : {}'.format(session.get_current_role()))\n",
    "print('Database                  : {}'.format(session.get_current_database()))\n",
    "print('Schema                    : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                 : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version         : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark-snowpark-python  : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n",
    "print('Snowflake-ml-python       : {}'.format(MLVersion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model with Snowpark ML and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from snowflake.ml.modeling.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "\n",
    "from snowflake.ml.modeling.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from snowflake.ml.modeling.impute import SimpleImputer\n",
    "from snowflake.ml.modeling.compose import ColumnTransformer\n",
    "\n",
    "# Create a session with your preferred method\n",
    "# session =\n",
    "\n",
    "NUMERICAL_COLS = [\"X1\", \"X2\", \"X3\"]\n",
    "CATEGORICAL_COLS = [\"C1\", \"C2\", \"C3\"]\n",
    "FEATURE_COLS = NUMERICAL_COLS + CATEGORICAL_COLS\n",
    "CATEGORICAL_OUTPUT_COLS = [\"C1_OUT\", \"C2_OUT\", \"C3_OUT\"]\n",
    "FEATURE_OUTPUT_COLS = [\"X1_FEAT_OUT\", \"X2_FEAT_OUT\", \"X3_FEAT_OUT\"]\n",
    "\n",
    "# Create a dataset with numerical and categorical features\n",
    "X, y = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=3,\n",
    "    noise=0.1,\n",
    "    random_state=0,\n",
    ")\n",
    "X = pd.DataFrame(X, columns=NUMERICAL_COLS)\n",
    "X['TARGET'] = y\n",
    "\n",
    "def generate_random_string(length):\n",
    "    return \"\".join(random.choices(string.ascii_uppercase, k=length))\n",
    "\n",
    "categorical_feature_length = 2\n",
    "categorical_features = {}\n",
    "for c in CATEGORICAL_COLS:\n",
    "    categorical_column = [generate_random_string(categorical_feature_length) for _ in range(X.shape[0])]\n",
    "    categorical_features[c] = categorical_column\n",
    "\n",
    "X = X.assign(**categorical_features)\n",
    "\n",
    "features_df = session.create_dataframe(X)\n",
    "\n",
    "# Fit a pipeline with OrdinalEncoder and MinMaxScaler on Snowflake\n",
    "numeric_features = [\"X1\", \"X2\", \"X3\"]\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "categorical_cols = [\"C1\", \"C2\", \"C3\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-99999))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', XGBRegressor())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180286</td>\n",
       "      <td>1.012168</td>\n",
       "      <td>-0.280448</td>\n",
       "      <td>-6.685136</td>\n",
       "      <td>ZJ</td>\n",
       "      <td>VW</td>\n",
       "      <td>ZI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.733595</td>\n",
       "      <td>2.011864</td>\n",
       "      <td>0.303011</td>\n",
       "      <td>261.472430</td>\n",
       "      <td>DP</td>\n",
       "      <td>VM</td>\n",
       "      <td>RM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.513903</td>\n",
       "      <td>-0.768849</td>\n",
       "      <td>0.988241</td>\n",
       "      <td>-52.651253</td>\n",
       "      <td>EB</td>\n",
       "      <td>FJ</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.037586</td>\n",
       "      <td>0.018792</td>\n",
       "      <td>1.392518</td>\n",
       "      <td>158.522375</td>\n",
       "      <td>YZ</td>\n",
       "      <td>JK</td>\n",
       "      <td>IH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.355029</td>\n",
       "      <td>-1.892362</td>\n",
       "      <td>-0.300479</td>\n",
       "      <td>-222.425070</td>\n",
       "      <td>IG</td>\n",
       "      <td>ZH</td>\n",
       "      <td>QJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.298760</td>\n",
       "      <td>-1.102230</td>\n",
       "      <td>0.699136</td>\n",
       "      <td>-85.555059</td>\n",
       "      <td>XI</td>\n",
       "      <td>RN</td>\n",
       "      <td>GM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.400157</td>\n",
       "      <td>0.978738</td>\n",
       "      <td>1.764052</td>\n",
       "      <td>224.479103</td>\n",
       "      <td>AO</td>\n",
       "      <td>WK</td>\n",
       "      <td>PV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.114335</td>\n",
       "      <td>0.743554</td>\n",
       "      <td>0.026247</td>\n",
       "      <td>63.462753</td>\n",
       "      <td>CC</td>\n",
       "      <td>ZB</td>\n",
       "      <td>VF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1.648135</td>\n",
       "      <td>0.164228</td>\n",
       "      <td>-1.471835</td>\n",
       "      <td>50.151860</td>\n",
       "      <td>JV</td>\n",
       "      <td>MS</td>\n",
       "      <td>KF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.454036</td>\n",
       "      <td>-0.256870</td>\n",
       "      <td>-0.780280</td>\n",
       "      <td>36.368242</td>\n",
       "      <td>PZ</td>\n",
       "      <td>VB</td>\n",
       "      <td>BE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3      TARGET  C1  C2  C3\n",
       "0   -1.180286  1.012168 -0.280448   -6.685136  ZJ  VW  ZI\n",
       "1    0.733595  2.011864  0.303011  261.472430  DP  VM  RM\n",
       "2   -0.513903 -0.768849  0.988241  -52.651253  EB  FJ  AN\n",
       "3    1.037586  0.018792  1.392518  158.522375  YZ  JK  IH\n",
       "4   -0.355029 -1.892362 -0.300479 -222.425070  IG  ZH  QJ\n",
       "..        ...       ...       ...         ...  ..  ..  ..\n",
       "995 -0.298760 -1.102230  0.699136  -85.555059  XI  RN  GM\n",
       "996  0.400157  0.978738  1.764052  224.479103  AO  WK  PV\n",
       "997 -0.114335  0.743554  0.026247   63.462753  CC  ZB  VF\n",
       "998  1.648135  0.164228 -1.471835   50.151860  JV  MS  KF\n",
       "999  1.454036 -0.256870 -0.780280   36.368242  PZ  VB  BE\n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.ml.modeling.model_selection.grid_search_cv.GridSearchCV at 0x16d864dd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## Distributed HyperParameter Optimization\n",
    "hyper_param = dict(\n",
    "        model__max_depth=[2,4],\n",
    "        model__learning_rate=[0.1,0.3],\n",
    "    )\n",
    "\n",
    "xg_model = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=hyper_param,\n",
    "    #cv=5,\n",
    "    input_cols= numeric_features + categorical_cols\n",
    ",    label_cols=['TARGET'],\n",
    "    output_cols=[\"TARGET_FORECAST\"],\n",
    ")\n",
    "\n",
    "# Fit and Score\n",
    "xg_model.fit(features_df)\n",
    "##Takes 25 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"X1\"                 |\"X2\"                   |\"X3\"                   |\"TARGET\"             |\"C1\"  |\"C2\"  |\"C3\"  |\"TARGET_FORECAST\"   |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "|-1.1802856063511906  |1.012168295174788      |-0.28044778146745414   |-6.685136199898421   |ZJ    |VW    |ZI    |3.5716018676757812  |\n",
      "|0.7335948682293696   |2.0118642631265615     |0.30301105045615745    |261.4724299705983    |DP    |VM    |RM    |255.3582763671875   |\n",
      "|-0.5139029502799155  |-0.7688491596748099    |0.9882405737426969     |-52.65125274455872   |EB    |FJ    |AN    |-43.4571533203125   |\n",
      "|1.037585667050634    |0.018791791774257802   |1.3925184494342724     |158.52237515962273   |YZ    |JK    |IH    |152.28465270996094  |\n",
      "|-0.3550287310553741  |-1.8923618933173414    |-0.3004787855854223    |-222.42507043560602  |IG    |ZH    |QJ    |-214.0787353515625  |\n",
      "|1.183119557331707    |0.7188971655282916     |1.4969109935208271     |241.49206454158585   |PH    |OO    |LG    |247.1415557861328   |\n",
      "|0.23903360124676537  |-1.0003303489537052    |-0.049324070147572695  |-80.07479163038643   |JN    |BA    |BF    |-86.10247802734375  |\n",
      "|0.46637957438197886  |-0.09439250641118496   |-0.4772862704032296    |-2.764902044444949   |FB    |EO    |PR    |-0.804442286491394  |\n",
      "|0.2456164279017758   |-0.032996480264666096  |0.8425887964569804     |63.95945110150179    |ZO    |AT    |LB    |55.94397735595703   |\n",
      "|0.5898798207345195   |-0.3638588099707899    |0.7679024077327039     |53.34135343700085    |MK    |CF    |NV    |30.989444732666016  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testpreds = xg_model.predict(features_df)\n",
    "testpreds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model with Snowpark ML and SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.ml.modeling.model_selection.grid_search_cv.GridSearchCV at 0x16c0bbe50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.ensemble import IsolationForest\n",
    "from snowflake.ml.modeling.ensemble import IsolationForest\n",
    "from snowflake.ml.modeling.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "# Define a custom scoring function\n",
    "scoring = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', IsolationForest())])\n",
    "#pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', RandomForestRegressor())])\n",
    "\n",
    "\n",
    " ## Distributed HyperParameter Optimization\n",
    "hyper_param = dict(\n",
    "        model__n_estimators = [20,50,200]\n",
    "       # model__learning_rate=[0.1,0.3],\n",
    "    )\n",
    "\n",
    "iso_model = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=hyper_param,\n",
    "    #cv=5,\n",
    "    scoring=scoring,\n",
    "    input_cols= numeric_features + categorical_cols,    \n",
    "  #  label_cols=['TARGET'],\n",
    "    output_cols=[\"TARGET_FORECAST\"],\n",
    ")\n",
    "\n",
    "# Fit and Score\n",
    "iso_model.fit(features_df)\n",
    "##Takes 25 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"X1\"                 |\"X2\"                   |\"X3\"                   |\"TARGET\"             |\"C1\"  |\"C2\"  |\"C3\"  |\"TARGET_FORECAST\"  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "|-1.1802856063511906  |1.012168295174788      |-0.28044778146745414   |-6.685136199898421   |ZJ    |VW    |ZI    |-1                 |\n",
      "|0.7335948682293696   |2.0118642631265615     |0.30301105045615745    |261.4724299705983    |DP    |VM    |RM    |-1                 |\n",
      "|-0.5139029502799155  |-0.7688491596748099    |0.9882405737426969     |-52.65125274455872   |EB    |FJ    |AN    |-1                 |\n",
      "|1.037585667050634    |0.018791791774257802   |1.3925184494342724     |158.52237515962273   |YZ    |JK    |IH    |1                  |\n",
      "|-0.3550287310553741  |-1.8923618933173414    |-0.3004787855854223    |-222.42507043560602  |IG    |ZH    |QJ    |1                  |\n",
      "|1.183119557331707    |0.7188971655282916     |1.4969109935208271     |241.49206454158585   |PH    |OO    |LG    |1                  |\n",
      "|0.23903360124676537  |-1.0003303489537052    |-0.049324070147572695  |-80.07479163038643   |JN    |BA    |BF    |1                  |\n",
      "|0.46637957438197886  |-0.09439250641118496   |-0.4772862704032296    |-2.764902044444949   |FB    |EO    |PR    |1                  |\n",
      "|0.2456164279017758   |-0.032996480264666096  |0.8425887964569804     |63.95945110150179    |ZO    |AT    |LB    |1                  |\n",
      "|0.5898798207345195   |-0.3638588099707899    |0.7679024077327039     |53.34135343700085    |MK    |CF    |NV    |1                  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testpreds = iso_model.predict(features_df)\n",
    "testpreds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Dataset for Stored Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "# Fetch the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "california_housing_df = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
    "california_housing_df['MEDHOUSEVAL'] = california_housing.target\n",
    "california_housing_df.columns = california_housing_df.columns.str.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('MEDINC', DoubleType(), nullable=True), StructField('HOUSEAGE', DoubleType(), nullable=True), StructField('AVEROOMS', DoubleType(), nullable=True), StructField('AVEBEDRMS', DoubleType(), nullable=True), StructField('POPULATION', DoubleType(), nullable=True), StructField('AVEOCCUP', DoubleType(), nullable=True), StructField('LATITUDE', DoubleType(), nullable=True), StructField('LONGITUDE', DoubleType(), nullable=True), StructField('MEDHOUSEVAL', DoubleType(), nullable=True)])\n"
     ]
    }
   ],
   "source": [
    "input_df = session.create_dataframe(california_housing_df)\n",
    "schema = input_df.schema\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.write.mode('overwrite').save_as_table('TRAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"MEDINC\"  |\"HOUSEAGE\"  |\"AVEROOMS\"          |\"AVEBEDRMS\"         |\"POPULATION\"  |\"AVEOCCUP\"          |\"LATITUDE\"  |\"LONGITUDE\"  |\"MEDHOUSEVAL\"  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|8.3252    |41.0        |6.984126984126984   |1.0238095238095237  |322.0         |2.5555555555555554  |37.88       |-122.23      |4.526          |\n",
      "|8.3014    |21.0        |6.238137082601054   |0.9718804920913884  |2401.0        |2.109841827768014   |37.86       |-122.22      |3.585          |\n",
      "|7.2574    |52.0        |8.288135593220339   |1.073446327683616   |496.0         |2.8022598870056497  |37.85       |-122.24      |3.521          |\n",
      "|5.6431    |52.0        |5.8173515981735155  |1.0730593607305936  |558.0         |2.547945205479452   |37.85       |-122.25      |3.413          |\n",
      "|3.8462    |52.0        |6.281853281853282   |1.0810810810810811  |565.0         |2.1814671814671813  |37.85       |-122.25      |3.422          |\n",
      "|4.0368    |52.0        |4.761658031088083   |1.1036269430051813  |413.0         |2.139896373056995   |37.85       |-122.25      |2.697          |\n",
      "|3.6591    |52.0        |4.9319066147859925  |0.9513618677042801  |1094.0        |2.1284046692607004  |37.84       |-122.25      |2.992          |\n",
      "|3.12      |52.0        |4.797527047913447   |1.061823802163833   |1157.0        |1.7882534775888717  |37.84       |-122.25      |2.414          |\n",
      "|2.0804    |42.0        |4.294117647058823   |1.1176470588235294  |1206.0        |2.026890756302521   |37.84       |-122.26      |2.267          |\n",
      "|3.6912    |52.0        |4.970588235294118   |0.9901960784313726  |1551.0        |2.172268907563025   |37.84       |-122.25      |2.611          |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = session.table('TRAIN')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stored Procedure for Training a Model with Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def housing_model(\n",
    "    session: Session, \n",
    "    features_table: str, \n",
    "    number_of_folds: int, \n",
    "    train_accuracy_threshold: float, \n",
    "    test_accuracy_threshold: float, \n",
    "    save_model: bool) -> Variant:\n",
    "    \n",
    "    import os\n",
    "\n",
    "    from joblib import dump\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "\n",
    "    from snowflake.ml.modeling.pipeline import Pipeline\n",
    "    from snowflake.ml.modeling.preprocessing import StandardScaler, OrdinalEncoder\n",
    "    from snowflake.ml.modeling.impute import SimpleImputer\n",
    "    from snowflake.ml.modeling.compose import ColumnTransformer\n",
    "\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "    # Load features\n",
    "    df = session.table('TRAIN').to_pandas()\n",
    "\n",
    "    # Preprocess the Numeric columns\n",
    "    # We apply PolynomialFeatures and StandardScaler preprocessing steps to the numeric columns\n",
    "    # NOTE: High degrees can cause overfitting.\n",
    "    numeric_features = ['MEDINC','HOUSEAGE','AVEROOMS','AVEBEDRMS','AVEOCCUP','POPULATION','LATITUDE','LONGITUDE']\n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "    # Combine the preprocessed step together using the Column Transformer module\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "    # The next step is the integrate the features we just preprocessed with our Machine Learning algorithm to enable us to build a model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', LinearRegression())])\n",
    "    parameteres = {}\n",
    "\n",
    "    X = df.drop('MEDHOUSEVAL', axis = 1)\n",
    "    y = df['MEDHOUSEVAL']\n",
    "\n",
    "    # Split dataset into training and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "    # Use GridSearch to find the best fitting model based on number_of_folds folds\n",
    "    model = GridSearchCV(pipeline, param_grid=parameteres, cv=number_of_folds)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    train_r2_score = model.score(X_train, y_train)\n",
    "    test_r2_score = model.score(X_test, y_test)\n",
    "\n",
    "    model_saved = False\n",
    "\n",
    "    if save_model:\n",
    "        if train_r2_score >= train_accuracy_threshold and test_r2_score >= test_accuracy_threshold:\n",
    "            # Upload trained model to a stage\n",
    "            model_output_dir = '/tmp'\n",
    "            model_file = os.path.join(model_output_dir, 'model.joblib')\n",
    "            dump(model, model_file)\n",
    "            session.file.put(model_file,\"@PYTHON_MODELS\",overwrite=True)\n",
    "            model_saved = True\n",
    "\n",
    "    # Return model R2 score on train and test data\n",
    "    return {\"R2 score on Train\": train_r2_score,\n",
    "            \"R2 threshold on Train\": train_accuracy_threshold,\n",
    "            \"R2 score on Test\": test_r2_score,\n",
    "            \"R2 threshold on Test\": test_accuracy_threshold,\n",
    "            \"Model saved\": model_saved}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R2 score on Train': 0.6125511913966952,\n",
       " 'R2 threshold on Train': 0.85,\n",
       " 'R2 score on Test': 0.5757877060324508,\n",
       " 'R2 threshold on Test': 0.85,\n",
       " 'Model saved': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validaton_folds = 3\n",
    "train_accuracy_threshold = 0.85\n",
    "test_accuracy_threshold = 0.85\n",
    "save_model = False\n",
    "\n",
    "housing_model(\n",
    "    session,\n",
    "    \"TRAIN\",\n",
    "    cross_validaton_folds,\n",
    "    train_accuracy_threshold,\n",
    "    test_accuracy_threshold,\n",
    "    save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x1686e4c10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sproc.register(\n",
    "    func=housing_model,\n",
    "    name=\"Train_housing_model\",\n",
    "    packages=['snowflake-ml-python','scikit-learn','joblib'],\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@MODELS\", ## I created this stage\n",
    "    replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Model saved\": false,\n",
      "  \"R2 score on Test\": 5.757877060324507e-01,\n",
      "  \"R2 score on Train\": 6.125511913966952e-01,\n",
      "  \"R2 threshold on Test\": 8.500000000000000e-01,\n",
      "  \"R2 threshold on Train\": 8.500000000000000e-01\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cross_validaton_folds = 3\n",
    "train_accuracy_threshold = 0.85\n",
    "test_accuracy_threshold = 0.85\n",
    "save_model = False\n",
    "\n",
    "print(session.call('Train_housing_model',\n",
    "                    'TRAIN',\n",
    "                    cross_validaton_folds,\n",
    "                    train_accuracy_threshold,\n",
    "                    test_accuracy_threshold,\n",
    "                    save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stored Procedure for Training a Model with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_housing_model(\n",
    "    session: Session, \n",
    "    features_table: str, \n",
    "    number_of_folds: int, \n",
    "    save_model: bool) -> Variant:\n",
    "    \n",
    "    import os\n",
    "\n",
    "    from joblib import dump\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "    from snowflake.ml.modeling.pipeline import Pipeline\n",
    "    from snowflake.ml.modeling.preprocessing import StandardScaler, OrdinalEncoder\n",
    "    from snowflake.ml.modeling.impute import SimpleImputer\n",
    "    from snowflake.ml.modeling.compose import ColumnTransformer\n",
    "\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Load features\n",
    "    df = session.table('TRAIN').to_pandas()\n",
    "    df = df.head(100) \n",
    "\n",
    "    # Preprocess the Numeric columns\n",
    "    # We apply PolynomialFeatures and StandardScaler preprocessing steps to the numeric columns\n",
    "    # NOTE: High degrees can cause overfitting.\n",
    "    numeric_features = ['MEDINC','HOUSEAGE','AVEROOMS','AVEBEDRMS','AVEOCCUP','POPULATION','LATITUDE','LONGITUDE']\n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "    # Combine the preprocessed step together using the Column Transformer module\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "    # The next step is the integrate the features we just preprocessed with our Machine Learning algorithm to enable us to build a model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', LinearRegression())])\n",
    "    parameteres = {}\n",
    "\n",
    "    X = df.drop('MEDHOUSEVAL', axis = 1)\n",
    "    y = df['MEDHOUSEVAL']\n",
    "\n",
    "    # Split dataset into training and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Define the smaller neural network model\n",
    "    class SmallerHousingModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SmallerHousingModel, self).__init__()\n",
    "            self.fc1 = nn.Linear(8, 64)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "            self.fc3 = nn.Linear(32, 1)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.relu = nn.ReLU()\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    # Instantiate the model, define the loss function and the optimizer\n",
    "    model = SmallerHousingModel()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        test_loss = criterion(test_outputs, y_test_tensor)\n",
    "        test_loss_float = test_loss.item()  #\n",
    "        print(f'Test Loss: {test_loss_float:.4f}')\n",
    "        \n",
    "    model_saved = False\n",
    "    test_loss = 0.2 #   if save_model:\n",
    " #         # Upload trained model to a stage\n",
    " #           model_output_dir = '/tmp'\n",
    " #           model_file = os.path.join(model_output_dir, 'model.joblib')\n",
    " #           dump(model, model_file)\n",
    " #           session.file.put(model_file,\"@PYTHON_MODELS\",overwrite=True)\n",
    "  #          model_saved = True\n",
    "\n",
    "    # Return model R2 score on train and test data\n",
    "    return {\"R2 score on Train\": test_loss_float,\n",
    "        \"Model saved\": model_saved}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 2.9312\n",
      "Test Loss: 3.8128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R2 score on Train': 3.8128409385681152, 'Model saved': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validaton_folds = 3\n",
    "save_model = False\n",
    "\n",
    "torch_housing_model(\n",
    "    session,\n",
    "    \"TRAIN\",\n",
    "    cross_validaton_folds,\n",
    "    save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x3041a88d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sproc.register(\n",
    "    func=torch_housing_model,\n",
    "    name=\"torch_housing_model\",\n",
    "    packages=['snowflake-ml-python','scikit-learn','joblib','pytorch'],\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@MODELS\", ## I created this stage\n",
    "    replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Model saved\": false,\n",
      "  \"R2 score on Train\": 2.532326459884644e+00\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cross_validaton_folds = 3\n",
    "save_model = False\n",
    "\n",
    "print(session.call('torch_housing_model',\n",
    "                    'TRAIN',\n",
    "                    cross_validaton_folds,\n",
    "                    save_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_public",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
