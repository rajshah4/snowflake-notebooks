{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd470ab5-0115-4cbd-b901-2a286ed5fadc",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "## OpenFE Short Demo / Tutorial\n",
    "\n",
    "A tool for automated feature engineering. It creates many features using common feature engineering techniques.\n",
    "I thought this was an interesting tool and wanted to do a quicker look. I am not an expert in OpenFE and there are many other great automated feature engineering tools.   \n",
    "\n",
    "Paper: https://arxiv.org/abs/2211.12507  \n",
    "Code: https://github.com/IIIS-Li-Group/OpenFE\n",
    "\n",
    "The notebook was exported from Snowflake, but should work in any compute environment. The only change will be ingesting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "!pip install openfe matplotlib -q\n",
    "\n",
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edb5c3-dc7b-4394-a024-41127bacb777",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "References:\n",
    "\n",
    "Using a Kaggle dataset on Mohs Hardness - Playground Series - Season 3, Episode 25: https://www.kaggle.com/competitions/playground-series-s3e25/data\n",
    "\n",
    "Useful Kaggle notebooks using OpenFE:\n",
    "Elevating Kaggle Performance with OpenFE - https://www.kaggle.com/code/sunilkumaradapa/elevating-kaggle-performance-with-openfe\n",
    "\n",
    "1st Place Solution for the Regression with an Abalone Dataset Competition - https://www.kaggle.com/competitions/playground-series-s4e4/discussion/499174\n",
    "\n",
    "OpenFE + Blending + Explain - https://www.kaggle.com/code/trupologhelper/ps4e5-openfe-blending-explain/notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed72f7-c0a1-46b6-9ce4-195b5f832735",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "train = session.read.table(\"RAJIV.KAGGLE.MOHS_TRAIN\").to_pandas()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41446f-5084-49dc-9edf-48d2f5568245",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "test = session.read.table(\"RAJIV.KAGGLE.MOHS_TEST\").to_pandas()\n",
    "test.head()\n",
    "test[\"IONENERGY_AVERAGE\"] = pd.to_numeric(test[\"IONENERGY_AVERAGE\"]) #something happened and this didn't get recognized as umeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029afcef-9d56-48c1-a558-cb707176b0b0",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "Feature engineering to select the features we want to use with OpenFE.You may need to use some of your expertise to exclude some features (leakage). \n",
    "But remember, some uninformative features may also yield informative candidate features after transformation. In a Diabetes dataset, for example, when the goal is to forecast if a patient will be readmitted to the hospital, the feature ‘patient id’ is useless. However, ‘freq(patient id)’, which is the number of times the patient has been admitted to the hospital, is a strong predictor of whether the patient would be readmitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547152c-9994-4658-9687-b2a608a806d6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "y = train['HARDNESS']\n",
    "X = train.drop(['ID','HARDNESS'],axis=1)\n",
    "X_test = test.drop(['ID'],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684ac77-feb1-45a5-bd3b-5ec1c027ce4a",
   "metadata": {
    "collapsed": false,
    "name": "cell16"
   },
   "source": [
    "Let's use OpenFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a2819-482c-4ce0-8b2b-73e3cc6fb9aa",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "from openfe import OpenFE, transform, tree_to_formula\n",
    "ofe = OpenFE()\n",
    "features = ofe.fit(data=X, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9fa63-ebb9-45ac-8eac-1b03aa7fece5",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "for feature in ofe.new_features_list[:10]:\n",
    "        print(tree_to_formula(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df2c80-fa96-4d2e-a478-5852823fce19",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "X_t, X_test = transform(X,X_test, features,n_jobs=4)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae06269-e881-442d-978d-e1d27640afa7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "X_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567cb59-4586-42d4-925b-8f78c1dd1fab",
   "metadata": {
    "collapsed": false,
    "name": "cell18"
   },
   "source": [
    "Advanced OpenFE\n",
    "\n",
    "When generating a lot of features, you will need more RAM, get a bigger instance. Or sample down your dataset (1% of it for example)\n",
    "\n",
    "You can increase `n_data_blocks` to speed up computation, but the quality of candidate features might be reduced\n",
    "\n",
    "Feature boosting is a more efficient way to identify the best featuers - I have seen people use OpenFE with and without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb8c4d-0ac7-45e5-888d-c8f8ffae62af",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "from openfe import OpenFE, transform, tree_to_formula\n",
    "\n",
    "n_jobs = 4\n",
    "params = {\"n_estimators\": 1000, \"importance_type\": \n",
    "          \"gain\", \"num_leaves\": 64,\n",
    "          \"seed\": 1, \"n_jobs\": n_jobs} ##for GBDT model by OpenFE\n",
    "\n",
    "ofe = OpenFE()\n",
    "features = ofe.fit(data=X, label=y, metric='rmse', \n",
    "                   task='regression', stage2_params=params,\n",
    "                   min_candidate_features=5000,n_jobs=n_jobs, \n",
    "                   n_data_blocks=2, feature_boosting=True)\n",
    "\n",
    "X_t2, test = transform(X, test, features,n_jobs=4)\n",
    "X_t2.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
