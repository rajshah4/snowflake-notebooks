{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChartQA Evaluation using Braintrust\n",
    "\n",
    "Chart Question & Answer is an increasing use cases as the capability of modern vision language models keeps increasing. Today's models can visually analyze documents and start to reason about them. To assess how well models are doing, I decided to analyze some models using the [ChartQA benchmark dataset](https://github.com/vis-nlp/ChartQA). \n",
    "\n",
    "Published benchmarks show GPT-4o getting about 85% accuracy. \n",
    "I wanted to run my own evaluation, where I could analyze the failure cases for the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install autoevals braintrust requests openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM clients\n",
    "\n",
    "We'll use OpenAI's GPT-4o against some of the ChartQA dataset. We will access these models\n",
    "behind the vanilla OpenAI client using Braintrust's proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braintrust\n",
    "import openai\n",
    "\n",
    "client = braintrust.wrap_openai(\n",
    "    openai.AsyncOpenAI(\n",
    "        api_key=os.environ[\"BRAINTRUST_API_KEY\"],\n",
    "        base_url=\"https://api.braintrust.dev/v1/proxy\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data and sanity testing it\n",
    "\n",
    "I pull the ChartQA dataset from Hugging Face hub at [lmms-lab/ChartQA](https://huggingface.co/datasets/lmms-lab/ChartQA)\n",
    "\n",
    "The datasets includes the question, answer, and image - let's test this out and see if we can query this data against GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Load ChartQA dataset\n",
    "dataset = load_dataset(\"lmms-lab/ChartQA\")\n",
    "\n",
    "# Function to load question, answer, and image from ChartQA\n",
    "def load_chart_qa_example(index):\n",
    "    example = dataset['test'][index]\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "    image_data = example['image']\n",
    "    \n",
    "    # Check if image_data is a URL or an image object\n",
    "    if isinstance(image_data, str):  # If it's a URL, fetch it\n",
    "        import requests\n",
    "        from io import BytesIO\n",
    "        image_response = requests.get(image_data)\n",
    "        image = Image.open(BytesIO(image_response.content))\n",
    "    elif isinstance(image_data, Image.Image):  # If it's already an image object\n",
    "        image = image_data\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected image data type.\")\n",
    "    \n",
    "    # Convert image to RGB if needed\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    \n",
    "    return question, answer, image\n",
    "\n",
    "# Example usage\n",
    "question, answer, image = load_chart_qa_example(0)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding LLM as a Judge Scorer\n",
    "A common problem with ChartQA is that model output is close, but not perfectly aligned with the corrrect answer. Let's add a LLM that will tell us if we are close to the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import Eval\n",
    "from autoevals import LLMClassifier\n",
    " \n",
    "partialc = LLMClassifier(\n",
    "    name=\"PartialCredit\",\n",
    "    prompt_template=\"You are going to judge the results of a QA task. The model answers on the basis of image and sometimes misses percentages, decimals, or other numerical transformations. You should ignore decimal places and percentage signs. If the answer is correct or similar/close to the answer, give partial credit. The scoring should be 0 for No credit and 1 for Full or Partial credit. An example of full credit would be an expected value of 3% and the output of 3 units \\n\\n Expected value: {{expected}} and output: {{output}}\",#prompt_template=\"You are going to judge the results of a QA task. Some of the results are returned as whole number, percentages, or decimals. If the answer is correct or similar/close to the answer, give a result iof partial credit: 0 for No credit, 1 for Full or Partial credit.  An example of partial credit would be an expected value of 3 and the output of 3% units \\n\\nExpected value {{expected}} compared to {{output}}\",\n",
    "    choice_scores={\"No\": 0, \"Partial\": 1},\n",
    "    use_cot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Models to evaluate\n",
    "MODELS = [\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\",\n",
    "  #  \"meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\",\n",
    "  #  \"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n",
    "  #  \"pixtral-12b-2409\",\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Answer the following question based on the provided image. \n",
    "Do not decorate the output with any explanation, or markdown. Just return the answer. \n",
    "{key}\n",
    "\"\"\"\n",
    "\n",
    "# Function to encode the image as base64 with a data URL prefix\n",
    "def encode_image(image):\n",
    "    from io import BytesIO\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    #return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return f\"data:image/png;base64,{base64.b64encode(buffered.getvalue()).decode('utf-8')}\"\n",
    "\n",
    "# Function to call the API with an image\n",
    "async def extract_value(model, key, base64_image):\n",
    "    # Add the data URL prefix within the API call\n",
    "    data_url = f\"data:image/png;base64,{base64_image}\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT.format(key=key)\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": key},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": base64_image}  # Add prefix here\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Example usage\n",
    "question, answer, image = load_chart_qa_example(0)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n",
    "image.show()\n",
    "\n",
    "# Encode the loaded image\n",
    "base64_image = encode_image(image)\n",
    "\n",
    "# Iterate over each model and print the response\n",
    "async def process_example(question):\n",
    "    for model in MODELS:\n",
    "        print(\"Running model:\", model)\n",
    "        result = await extract_value(model, question, base64_image)\n",
    "        print(\"Model:\", model, \"| Answer:\", result, \"\\n\")\n",
    "\n",
    "# Run with the example question\n",
    "await process_example(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the evaluation\n",
    "\n",
    "Now that we were able to perform a basic sanity test, let's run an evaluation! We'll use the `Levenshtein` and `Factuality` scorers to assess performance.\n",
    "`Levenshtein` is heuristic and will tell us how closely the actual and expected strings match. Assuming some of the models will occasionally spit out superfluous\n",
    "explanation text, `Factuality`, which is LLM based, should be able to still give us an accuracy measurement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import Eval\n",
    "from autoevals import Factuality, Levenshtein\n",
    "\n",
    "NUM_EXAMPLES = 100\n",
    "\n",
    "# Prepare data with base64-encoded images instead of img_path\n",
    "data = []\n",
    "for idx in range(NUM_EXAMPLES):\n",
    "    question, answer, image = load_chart_qa_example(idx)\n",
    "    base64_image = encode_image(image)  # Encode the image to base64\n",
    "    \n",
    "    data.append({\n",
    "        \"input\": {\n",
    "            \"key\": question,\n",
    "            \"img_data\": base64_image,\n",
    "        },\n",
    "        \"expected\": answer,\n",
    "        \"metadata\": {\n",
    "            \"idx\": idx,\n",
    "        },\n",
    "    })\n",
    "\n",
    "# Run evaluation for each model\n",
    "for model in MODELS:\n",
    "\n",
    "    async def task(input):\n",
    "        # Use `img_data` as the encoded image\n",
    "        return await extract_value(model, input[\"key\"], input[\"img_data\"])\n",
    "\n",
    "    await Eval(\n",
    "        \"ChartQA Extraction\",\n",
    "        data=data,\n",
    "        task=task,\n",
    "        scores=[Levenshtein, Factuality,partialc],\n",
    "        experiment_name=f\"ChartQA Extraction - {model}\",\n",
    "        metadata={\"model\": model},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Takeaways from Using Braintrust\n",
    "\n",
    "- I could use multiple models - I am passing the image directly, so some other vision models, such as from together would require me reworking the datasets into an image URL\n",
    "\n",
    "- I was able to evaluate a vision lanaguage model\n",
    "\n",
    "- Easy to run across multiple modes\n",
    "\n",
    "- Easy to add my own scorer - LLM as a judge\n",
    "\n",
    "- I could follow improvements / regressions \n",
    "\n",
    "- Did this all from a notebook (could have set this up through the UI)\n",
    "\n",
    "- Evalaution is easy to drill into - see specific examples, see the actual text passed to the model \n",
    "\n",
    "- Was able to see how my custom model did\n",
    "\n",
    "- Obviously, many more comparisons that i could do - lot more for the tool here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
